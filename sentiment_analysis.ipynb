{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_df(filename):\n",
    "    # Parse xml file into pandas data frame to work with at the sentence level\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    reviews = root.findall('Review')\n",
    "    df_columns = ['rid','id','text','category','predicted_category','polarity','predicted_polarity']\n",
    "    reviews_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "    for review in reviews:\n",
    "        rid = review.get('rid')\n",
    "        sentences = review.findall('sentences/sentence')\n",
    "        for sentence in sentences:\n",
    "            id = sentence.get('id')\n",
    "            text = sentence.find('text').text\n",
    "            opinions = sentence.findall('Opinions/Opinion')\n",
    "            for opinion in opinions:\n",
    "                category = opinion.get('category')\n",
    "                polarity = opinion.get('polarity')\n",
    "                predicted_category = ''\n",
    "                predicted_polarity = ''\n",
    "                reviews_df = pd.concat([reviews_df, \n",
    "                            pd.DataFrame([[rid, id, text, category, \n",
    "                            predicted_category, polarity,predicted_polarity]],\n",
    "                            columns=df_columns)], ignore_index=True)\n",
    "    \n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews_df = xml_to_df(filename='Laptops_Train_p1.xml')\n",
    "train_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) __Text processing__\n",
    "- Create copy of original data frame, populate with processed text\n",
    "- tokenise words\n",
    "- remove stop words\n",
    "- remove punctuation\n",
    "- stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = train_reviews_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    # Tokenising sentences\n",
    "    tokenised_text = [word_tokenize(sentence.lower()) for sentence in text]\n",
    "    return tokenised_text\n",
    "\n",
    "def remove_stopwords(tokenised_text):\n",
    "    # Remove stop words\n",
    "    tokens = []\n",
    "    for token in tokenised_text:\n",
    "        if token not in stopwords.words('english'):\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def remove_non_alpha(tokenised_text):\n",
    "    # Remove punctuation\n",
    "    alpha_tokens = []\n",
    "    for token in tokenised_text:\n",
    "        if token.isalpha():\n",
    "            alpha_tokens.append(token)\n",
    "    return alpha_tokens\n",
    "\n",
    "def stem(tokenised_text):\n",
    "    # Stem tokenised text\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    stem_tokens = []\n",
    "    for token in tokenised_text:\n",
    "        stem_tokens.append(snow_stemmer.stem(token))\n",
    "  \n",
    "    stemmed_text = \" \".join(stem_tokens)\n",
    "    return stemmed_text\n",
    "\n",
    "def preprocess(tokenised_text):\n",
    "  # output processed text\n",
    "  pp_data = []\n",
    "  for sentence in tokenised_text:\n",
    "    pp_text = remove_stopwords(sentence)\n",
    "    pp_text = remove_non_alpha(pp_text)\n",
    "    pp_text = stem(pp_text)\n",
    "    pp_data.append(pp_text)\n",
    "  return pp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['text'] = preprocess(tokenise(train_reviews_df['text']))\n",
    "processed_df = processed_df.rename(columns={\"text\": \"processed_text\"})\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_a_predict(features, X_train_counts, model, threshold):\n",
    "    # Returns the next most likely topic if a given sentence has multiple categories\n",
    "    # if probability for next most likely topic is > threshold, else return most likely topic.\n",
    "\n",
    "    N = len(features)\n",
    "    tmp = 0\n",
    "    repeat = 0\n",
    "    predictions = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if features[i] != tmp:\n",
    "            predictions[i] = model.predict(X_train_counts[i,:])\n",
    "            repeat = 0\n",
    "        else:\n",
    "            repeat += 1\n",
    "            arr = model.predict_proba(X_train_counts[i,:])\n",
    "            sorted_index = np.argsort(arr)[0]\n",
    "            if arr[0][sorted_index[-repeat-1]] > threshold/repeat:\n",
    "                predictions[i] = float(sorted_index[-repeat-1])\n",
    "            else:\n",
    "                predictions[i] = model.predict(X_train_counts[i,:])\n",
    "            # handle part 2 error where >8 opinions for single review gives index error\n",
    "            if repeat > 7:\n",
    "                repeat -= 1\n",
    "        tmp = features[i]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def sorted_predictions(df):\n",
    "    # Aligns predictions with matching labels for sentences that have multiple opinions.\n",
    "    # e.g., ground truth for sentence id=1: LAPTOP#GENERAL, LAPTOP#BATTERY_PERFORMANCE\n",
    "    # predictions for sentence id=1 pre-alignment: LAPTOP#BATTERY_PERFORMANCE, LAPTOP#GENERAL\n",
    "    # predictions post-alignment: LAPTOP#GENERAL, LAPTOP#BATTERY_PERFORMANCE\n",
    "\n",
    "    N = len(df['category'])\n",
    "    labels_dict = {}\n",
    "    predictions_dict = {}\n",
    "    sorted_predictions = []\n",
    "    i = 0\n",
    "    tmp = 0\n",
    "\n",
    "    for id in df['id'].unique():\n",
    "        labels_dict[id] = list(df.query(f\"id == '{id}'\")['category'])\n",
    "        predictions_dict[id] = list(df.query(f\"id == '{id}'\")['predicted_category'])\n",
    "\n",
    "    for key in labels_dict.keys():\n",
    "        for value in labels_dict[key]:\n",
    "            if value in predictions_dict[key]:\n",
    "\n",
    "                idx = labels_dict[key].index(value) # obtain label index for matching label & prediction for given sentence id\n",
    "                idx2 = predictions_dict[key].index(value) # obtain prediction index for matching label & prediction for given sentence id\n",
    "\n",
    "                tmp = predictions_dict[key][idx]\n",
    "\n",
    "                predictions_dict[key][idx] = value # re-order predictions so that they align with ground truth for given id\n",
    "                predictions_dict[key][idx2] = tmp # swap changed values in predictions list for sentence id\n",
    "    \n",
    "    for values in predictions_dict.values():\n",
    "        for value in values:\n",
    "            sorted_predictions.append(value)\n",
    "\n",
    "    return list(sorted_predictions)\n",
    "\n",
    "def numerical_entity_attributes(df):\n",
    "    # Numerical representation of predicted categories is required for accuracy\n",
    "    # and classification report. Function converts category predictions to\n",
    "    # numerical representation\n",
    "\n",
    "    category, predicted_category = df['category'], df['predicted_category']\n",
    "\n",
    "    e_a_list = category.unique().tolist()\n",
    "    i = 0\n",
    "    e_a_label_dict = {}\n",
    "    for e_a in e_a_list:\n",
    "        e_a_label_dict[e_a] = i\n",
    "        i += 1\n",
    "\n",
    "    e_a_labels = []\n",
    "    for i in range(len(category)):\n",
    "        e_a_labels.append(e_a_label_dict[category[i]])\n",
    "\n",
    "    e_a_predictions = []\n",
    "    for i in range(len(category)):\n",
    "        if predicted_category[i] in e_a_list:\n",
    "            e_a_predictions.append(e_a_label_dict[predicted_category[i]])\n",
    "        else:\n",
    "            e_a_predictions.append(99) # predicted E#A pair not in ground truth set of E#A pairs\n",
    "\n",
    "    e_a_labels = np.array(e_a_labels, dtype=int)\n",
    "    e_a_predictions = np.array(e_a_predictions, dtype=int)\n",
    "    e_a_list.append('N/A')\n",
    "\n",
    "    return e_a_labels, e_a_predictions, e_a_list, e_a_label_dict\n",
    "\n",
    "def reverse_dict(my_dict):\n",
    "    # Reverses the keys and values in a dictionary, useful for several later steps\n",
    "    reversed_dict = {}\n",
    "    for key, value in my_dict.items(): \n",
    "        reversed_dict[value] = key \n",
    "    return reversed_dict\n",
    "\n",
    "def numerical_labels(df, label_name, new_column_name=str()):\n",
    "    # Convert ground truth word labels for entities or attributes to numerical representation\n",
    "    labels_list = df[label_name].unique().tolist()\n",
    "    df[new_column_name] = ''\n",
    "    i = 0\n",
    "    labels_dict = {} # Will use this to convert numerical class predictions back to attributes\n",
    "    for label in labels_list:\n",
    "        df.loc[df[label_name] == label, new_column_name] = i\n",
    "        labels_dict[i] = label\n",
    "        i += 1\n",
    "    \n",
    "    return labels_list, labels_dict\n",
    "\n",
    "def convert_numerical_predictions(num_predictions, label_dict):\n",
    "    # Convert numerical predictions back to words\n",
    "    num_predictions = num_predictions.tolist()\n",
    "    word_predictions = []\n",
    "\n",
    "    for pred in num_predictions:\n",
    "        word_predictions.append(label_dict[pred])\n",
    "    \n",
    "    return word_predictions\n",
    "\n",
    "def combine_entity_attributes(df, entity_pred, attrib_pred):\n",
    "    # Combine Entity and Attribute predictions to create E#A pair predictions\n",
    "    # Sort E#A pair category predictions\n",
    "    combined_list = []\n",
    "    for i in range(len(df['category'])):\n",
    "        e_a_pair = entity_pred[i] + \"#\" + attrib_pred[i]\n",
    "        combined_list.append(e_a_pair)\n",
    "    \n",
    "    return combined_list\n",
    "\n",
    "def overall_accuracy_absa(df):\n",
    "    # Returns an overall accuracy score as a % of the instances where both the\n",
    "    # category predictions AND the sentiment predicitons were correct.\n",
    "    N = len(df['category'])\n",
    "    correct_count = 0\n",
    "    for i in range(N):\n",
    "        if df['category'][i] == df['predicted_category'][i] and df['polarity'][i] == df['predicted_polarity'][i]:\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = (correct_count/N)*100\n",
    "    output = f\"Overall accuracy for correct category and sentiment predictions: {accuracy:.0f}%\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating training features for entity and attribute classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = processed_df['processed_text']\n",
    "# Using counts to extract features from text\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training topic classifier first for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate entities from categories and add to new column\n",
    "processed_df['entity'] = [entity.split('#')[0] for entity in processed_df['category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label entities\n",
    "entities_list, entity_label_dict = numerical_labels(processed_df, 'entity', 'entity_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set for entity classification\n",
    "Y_entity_train = processed_df['entity_label']\n",
    "Y_entity_train = np.array(Y_entity_train, dtype=int)\n",
    "\n",
    "# Train the entity classification Logistic Regression model\n",
    "entity_count_lr = LogisticRegression()\n",
    "entity_count_lr.fit(X_train_counts, Y_entity_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing entity prediction on training data... overfitting likely\n",
    "Y_entity_train_pred = e_a_predict(X_train,X_train_counts,model=entity_count_lr,threshold=0.1) # threshold hyperparameter tuned on training data\n",
    "entity_train_accuracy = accuracy_score(Y_entity_train, Y_entity_train_pred)\n",
    "\n",
    "print(f\"Training set entity predicitons accuracy score: {entity_train_accuracy*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert entity predictions from numbers back to words\n",
    "entity_predictions = convert_numerical_predictions(Y_entity_train_pred, entity_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training topic classifier for attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate attributes from categories and add to new column\n",
    "processed_df['attribute'] = [attribute.split('#')[1] for attribute in processed_df['category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label attributes\n",
    "attributes_list, attribute_label_dict = numerical_labels(processed_df, 'attribute', 'attribute_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set for attribute classification\n",
    "Y_attrib_train = processed_df['attribute_label']\n",
    "Y_attrib_train = np.array(Y_attrib_train, dtype=int)\n",
    "\n",
    "# Train the attribute classification Logistic Regression model\n",
    "attrib_count_lr = LogisticRegression()\n",
    "attrib_count_lr.fit(X_train_counts, Y_attrib_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing attribute prediction on training data... overfitting likely\n",
    "Y_attrib_train_pred = e_a_predict(X_train,X_train_counts,model=attrib_count_lr,threshold=0.1) # threshold hyperparameter tuned on training data\n",
    "attrib_train_accuracy = accuracy_score(Y_attrib_train, Y_attrib_train_pred)\n",
    "\n",
    "print(f\"Training set attribute predictions accuracy score: {attrib_train_accuracy*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert attribute predictions from numbers back to words\n",
    "attribute_predictions = convert_numerical_predictions(Y_attrib_train_pred, attribute_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Entity and Attribute predictions to create E#A pair predictions\n",
    "processed_df['predicted_category'] = combine_entity_attributes(processed_df, entity_predictions, attribute_predictions)\n",
    "\n",
    "# Sort Entity and Attribute predictions for more representative accuracy score (see example in function comments)\n",
    "processed_df['predicted_category'] = sorted_predictions(processed_df)\n",
    "train_reviews_df['predicted_category'] = sorted_predictions(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_a_labels, e_a_predictions, e_a_list, e_a_label_dict  = numerical_entity_attributes(processed_df)\n",
    "category_train_accuracy = accuracy_score(e_a_labels, e_a_predictions)\n",
    "\n",
    "print(f\"Training set category predictions accuracy score: {category_train_accuracy*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "reorder_columns = ['rid','id','processed_text','entity','entity_label',\n",
    "                  'attribute','attribute_label','category','predicted_category',\n",
    "                  'polarity','predicted_polarity']\n",
    "processed_df = processed_df.reindex(columns=reorder_columns)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_text(text):\n",
    "    # Split input sentence/review by by coordinating conjunctions e.g., ['and','but','because']\n",
    "    # If no CC in sentence, then split by punctuation\n",
    "    # Each element (part of sentence) can then be assigned to different categories for the same sentence id, \n",
    "    # based on cosine similarity measure\n",
    "\n",
    "    split_input = []\n",
    "    for sentence in text:\n",
    "        sentence = sentence.lower()\n",
    "        sent_tag = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "        split_words = []\n",
    "        pos_tags = ['CC']\n",
    "        for elem in sent_tag:\n",
    "            if elem[1] in pos_tags:\n",
    "                split_words.append(elem[0])\n",
    "        \n",
    "        punctuation = [',',';','-']\n",
    "        cc_in_flag = False\n",
    "        for elem in sent_tag:\n",
    "            if elem[1] in pos_tags:\n",
    "                cc_in_flag = True\n",
    "        \n",
    "        if not cc_in_flag:\n",
    "            for elem in sent_tag:\n",
    "                if elem[1] in punctuation:\n",
    "                    split_words.append(elem[0])\n",
    "\n",
    "        result = []\n",
    "        if split_words:\n",
    "            for index, word in enumerate(split_words):\n",
    "                result.append(sentence.split(split_words[index])[0])\n",
    "                if len(sentence.split(split_words[index]))>1:\n",
    "                    sentence = sentence.split(split_words[index])[1]\n",
    "            result.append(sentence)\n",
    "        else:\n",
    "            result.append(sentence)\n",
    "\n",
    "        result = [item.strip() for item in result]\n",
    "        result = [item for item in result if item]\n",
    "        split_input.append(result)\n",
    "        \n",
    "    return split_input\n",
    "\n",
    "def filter_sentiment_input(text, cat):\n",
    "    # Keep only most relevant element in sentence list to determine polarity for category\n",
    "    # Measuring similarity between nouns in element of sentence and category (E#A) pair\n",
    "    # Less expensive (more efficient) similarity calculation using only nouns rather than all words in part of sentence\n",
    "    reduced_text = []\n",
    "    for index, row in enumerate(text):\n",
    "        n = len(row)\n",
    "        tmp_dict = {}\n",
    "        for i in range(n):\n",
    "            cat_list = re.split('_|#', cat.loc[index])\n",
    "            cat_list = [item.title() if item != 'OS' else item for item in cat_list]\n",
    "            category = nlp(' '.join(cat_list))\n",
    "\n",
    "            sent_tag = nltk.pos_tag(nltk.word_tokenize(row[i].lower()))\n",
    "            noun_pos = ['NN','NNS','NNP']\n",
    "            noun_list = [elem[0] for elem in sent_tag if elem[1] in noun_pos]\n",
    "            noun_text = nlp(' '.join(noun_list))\n",
    "\n",
    "            tmp_dict[row[i]] = category.similarity(noun_text)\n",
    "\n",
    "        reduced_text.append(max(tmp_dict, key = tmp_dict.get))\n",
    "\n",
    "    return reduced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label sentiments\n",
    "sentiments_list, polarity_label_dict = numerical_labels(processed_df, 'polarity', 'polarity_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set for sentiment classification\n",
    "Y_sentiment_train = processed_df['polarity_label']\n",
    "Y_sentiment_train = np.array(Y_sentiment_train, dtype=int)\n",
    "\n",
    "# Train the attribute classification Naive Bayes model\n",
    "sentiment_count_lr = LogisticRegression()\n",
    "sentiment_count_lr.fit(X_train_counts, Y_sentiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_processed_df = train_reviews_df.copy()\n",
    "sentiment_processed_df['text'] = split_input_text(train_reviews_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B. cell takes ~30secs to run\n",
    "sentiment_processed_df['text'] = filter_sentiment_input(sentiment_processed_df['text'], sentiment_processed_df['predicted_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_processed_df.query(\"id == '273:9'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input for sentiment prediction on training set\n",
    "sentiment_processed_df['processed_text'] = preprocess(tokenise(sentiment_processed_df['text']))\n",
    "\n",
    "X_sentiment_input = sentiment_processed_df['processed_text']\n",
    "X_sentiment_input = count_vectorizer.transform(X_sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing sentiment prediction on training data... overfitting likely\n",
    "Y_sentiment_train_pred = sentiment_count_lr.predict(X_sentiment_input)\n",
    "\n",
    "sentiment_train_accuracy = accuracy_score(Y_sentiment_train, Y_sentiment_train_pred)\n",
    "print(f\"Training set sentiment prediction accuracy score: {sentiment_train_accuracy*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(Y_sentiment_train, Y_sentiment_train_pred, target_names=sentiments_list)\n",
    "# print(cr)\n",
    "# Note that only 188 neutral training examples out of 2909... will be difficult to learn properties of neutral input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment predictions from numbers back to words\n",
    "sentiment_predictions = convert_numerical_predictions(Y_sentiment_train_pred, polarity_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['predicted_polarity'] = sentiment_predictions\n",
    "reorder_columns = ['rid','id','processed_text','entity','entity_label',\n",
    "                  'attribute','attribute_label','category','predicted_category',\n",
    "                  'polarity','polarity_label','predicted_polarity']\n",
    "processed_df = processed_df.reindex(columns=reorder_columns)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews_df['predicted_polarity'] = sentiment_predictions\n",
    "train_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews_df = xml_to_df(filename='Laptops_Test_p1_gold.xml')\n",
    "test_processed_df = test_reviews_df.copy()\n",
    "test_reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-processing test set text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process test set text\n",
    "test_processed_df['text'] = preprocess(tokenise(test_reviews_df['text']))\n",
    "\n",
    "# Obtain test set input features\n",
    "X_test = test_processed_df['text']\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make Entity predictions using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set entities\n",
    "Y_entity_test_pred = e_a_predict(X_test,X_test_counts,model=entity_count_lr,threshold=0.1) # threshold hyperparameter tuned on training data\n",
    "\n",
    "# Convert entity predictions back to words\n",
    "test_entity_predictions = convert_numerical_predictions(Y_entity_test_pred, entity_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make Attribute predictions using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set attributes\n",
    "Y_attrib_test_pred = e_a_predict(X_test,X_test_counts,model=attrib_count_lr,threshold=0.1) # threshold hyperparameter tuned on training data\n",
    "\n",
    "# Convert attribute predictions back to words\n",
    "test_attribute_predictions = convert_numerical_predictions(Y_attrib_test_pred, attribute_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Entity and Attribute predictions to create E#A pair predictions\n",
    "test_processed_df['predicted_category'] = combine_entity_attributes(test_processed_df, test_entity_predictions, test_attribute_predictions)\n",
    "\n",
    "# Sort Entity and Attribute predictions for more representative accuracy score (see example in function comments)\n",
    "test_processed_df['predicted_category'] = sorted_predictions(test_processed_df)\n",
    "test_reviews_df['predicted_category'] = sorted_predictions(test_processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Produce Classification report for E#A pair predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_a_labels, e_a_predictions, e_a_list, e_a_label_dict = numerical_entity_attributes(test_processed_df)\n",
    "\n",
    "# Note target_names=e_a_list displays E#A categories as text form rather than numerical representation\n",
    "cr = classification_report(e_a_labels, e_a_predictions, target_names=e_a_list)\n",
    "print(\"Predicted E#A Category Classification Report (Test Set):\\n\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification report for sentiment predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversing the polarity label dict to faciliatate test set polarity conversion to numerical labels.\n",
    "reversed_polarity_label_dict = reverse_dict(polarity_label_dict)\n",
    "\n",
    "Y_sentiment_test_labels = [reversed_polarity_label_dict[test_processed_df['polarity'][i]] for i in range(len(test_processed_df['polarity']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing filtered input for aspect based sentiment predictions on test set\n",
    "test_sentiment_prep_df = test_reviews_df.copy()\n",
    "test_sentiment_prep_df['text'] = split_input_text(test_reviews_df['text'])\n",
    "\n",
    "test_sentiment_prep_df['processed_text'] = preprocess(tokenise(filter_sentiment_input(test_sentiment_prep_df['text'], test_sentiment_prep_df['predicted_category'])))\n",
    "X_test_sentiment_input = count_vectorizer.transform(test_sentiment_prep_df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifcation report for sentiment precitions on test data\n",
    "Y_sentiment_test_pred = sentiment_count_lr.predict(X_test_sentiment_input)\n",
    "Y_sentiment_test_labels = np.array([Y_sentiment_test_labels]).reshape(801,)\n",
    "\n",
    "cr = classification_report(Y_sentiment_test_labels, Y_sentiment_test_pred, target_names=sentiments_list)\n",
    "print(\"Predicted Sentiment Classification Report (Test Set):\\n\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment predictions to words and add to test test reviews data frame\n",
    "test_processed_df['predicted_polarity'] = convert_numerical_predictions(Y_sentiment_test_pred, polarity_label_dict)\n",
    "test_reviews_df['predicted_polarity'] = convert_numerical_predictions(Y_sentiment_test_pred, polarity_label_dict)\n",
    "\n",
    "test_reviews_df[['text','category','predicted_category','polarity','predicted_polarity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the % of predictions where both the category AND polarity were predicted correct\n",
    "combined_accuracy = overall_accuracy_absa(test_reviews_df)\n",
    "print(\"For Part 1 test data:\")\n",
    "print(combined_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse xml file into pandas data frame to work with at the review level\n",
    "def part2_xml_to_df(filename):\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    reviews = root.findall('Review')\n",
    "    df_columns = ['rid','text','category','predicted_category','polarity','predicted_polarity']\n",
    "    reviews_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "    for review in reviews:\n",
    "        rid = review.get('rid')\n",
    "        sentences = review.findall('sentences/sentence')\n",
    "        text = ''\n",
    "        for sentence in sentences:\n",
    "            text += sentence.find('text').text + ' '\n",
    "        opinions = review.findall('Opinions/Opinion')\n",
    "        for opinion in opinions:\n",
    "            category = opinion.get('category')\n",
    "            polarity = opinion.get('polarity')\n",
    "            predicted_category = ''\n",
    "            predicted_polarity = ''\n",
    "            reviews_df = pd.concat([reviews_df, \n",
    "                        pd.DataFrame([[rid, text, category, predicted_category, polarity, predicted_polarity]], \n",
    "                        columns=df_columns)], ignore_index=True)\n",
    "    \n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews_df_p2 = part2_xml_to_df(filename='Laptops_Train_p2.xml')\n",
    "train_reviews_df_p2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) __Processing__\n",
    "- Create copy of original data frame, to work with to determine classifications\n",
    "- Preprocess review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_p2 = train_reviews_df_p2.copy()\n",
    "processed_df_p2['text'] = preprocess(tokenise(train_reviews_df_p2['text']))\n",
    "processed_df_p2 = processed_df_p2.rename(columns={\"text\": \"processed_text\"})\n",
    "processed_df_p2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining functions used to obtain category predictions in Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_predictions_p2(df):\n",
    "    # Updated for Part 2 to query unique review id isntead of sentence id\n",
    "    # Aligns predictions with matching labels for sentences that have multiple opinions.\n",
    "    # e.g., ground truth for sentence id=1: LAPTOP#GENERAL, LAPTOP#BATTERY_PERFORMANCE\n",
    "    # predictions for sentence id=1 pre-alignment: LAPTOP#BATTERY_PERFORMANCE, LAPTOP#GENERAL\n",
    "    # predictions post-alignment: LAPTOP#GENERAL, LAPTOP#BATTERY_PERFORMANCE\n",
    "\n",
    "    N = len(df['category'])\n",
    "    labels_dict = {}\n",
    "    predictions_dict = {}\n",
    "    sorted_predictions = []\n",
    "    i = 0\n",
    "    tmp = 0\n",
    "\n",
    "    for rid in df['rid'].unique():\n",
    "        labels_dict[rid] = list(df.query(f\"rid == '{rid}'\")['category'])\n",
    "        predictions_dict[rid] = list(df.query(f\"rid == '{rid}'\")['predicted_category'])\n",
    "\n",
    "    for key in labels_dict.keys():\n",
    "        for value in labels_dict[key]:\n",
    "            if value in predictions_dict[key]:\n",
    "\n",
    "                idx = labels_dict[key].index(value) # obtain label index for matching label & prediction for given sentence id\n",
    "                idx2 = predictions_dict[key].index(value) # obtain prediction index for matching label & prediction for given sentence id\n",
    "\n",
    "                tmp = predictions_dict[key][idx]\n",
    "\n",
    "                predictions_dict[key][idx] = value # re-order predictions so that they align with ground truth for given id\n",
    "                predictions_dict[key][idx2] = tmp # swap changed values in predictions list for sentence id\n",
    "    \n",
    "    for values in predictions_dict.values():\n",
    "        for value in values:\n",
    "            sorted_predictions.append(value)\n",
    "\n",
    "    return list(sorted_predictions)\n",
    "\n",
    "def identify_p2_categories(df_p1, df_p2):\n",
    "    # Identify p2 categories given category predictions for sentences with \n",
    "    # matching review id from part 1\n",
    "    # Additional functionality: if LAPTOP#GENERAL not predicted, remove least\n",
    "    # similar predicted category and include LAPTOP#GENERAL as predicted category\n",
    "    # If same category predicted more than once, remove duplicate and predict new\n",
    "\n",
    "    categories = []\n",
    "    pred_cat = 'predicted_category'\n",
    "    for rid in df_p2['rid'].unique():\n",
    "        len_pred_cats_p1 = len(df_p1.query(f'rid == \"{rid}\"')[pred_cat])\n",
    "        len_pred_cats_p2 = len(df_p2.query(f'rid == \"{rid}\"')[pred_cat])\n",
    "        if 'LAPTOP#GENERAL' not in set(df_p1.query(f'rid == \"{rid}\"')[pred_cat]):\n",
    "            if len_pred_cats_p2 < len_pred_cats_p1:\n",
    "                categories += ['LAPTOP#GENERAL']\n",
    "                categories += list(df_p1.query(f'rid == \"{rid}\"')[pred_cat])[:len_pred_cats_p2-1]\n",
    "            elif len_pred_cats_p1 == len_pred_cats_p2:\n",
    "                categories += ['LAPTOP#GENERAL']\n",
    "                categories += list(df_p1.query(f'rid == \"{rid}\"')[pred_cat])[:len_pred_cats_p2-1]\n",
    "            else:  \n",
    "                delta = len_pred_cats_p2 - len_pred_cats_p1\n",
    "                categories += ['LAPTOP#GENERAL']\n",
    "                categories += list(df_p1.query(f'rid == \"{rid}\"')[pred_cat])\n",
    "                for i in range(delta-1):\n",
    "                    categories += ['LAPTOP#GENERAL'] # duplicate predictions will be handled later\n",
    "\n",
    "        else:\n",
    "            if len_pred_cats_p2 < len_pred_cats_p1:\n",
    "                if 'LAPTOP#GENERAL' in list(df_p1.query(f'rid == \"{rid}\"')[pred_cat])[:len_pred_cats_p2]:\n",
    "                    categories += list(df_p1.query(f'rid == \"{rid}\"')[pred_cat])[:len_pred_cats_p2]\n",
    "                else:\n",
    "                    categories += ['LAPTOP#GENERAL']\n",
    "                    categories += list(df_p1.query(f'rid == \"{rid}\"')[pred_cat])[:len_pred_cats_p2-1]\n",
    "            elif len_pred_cats_p1 == len_pred_cats_p2:\n",
    "                categories += list(df_p1.query(f'rid == \"{rid}\"')[pred_cat])\n",
    "            else:\n",
    "                delta = len_pred_cats_p2 - len_pred_cats_p1\n",
    "                categories += list(df_p1.query(f'rid == \"{rid}\"')['predicted_category'])\n",
    "                for i in range(delta):\n",
    "                    categories += ['LAPTOP#GENERAL'] # duplicate predictions will be handled later\n",
    "\n",
    "    return categories\n",
    "\n",
    "def repredict_duplicate_cats(process_df, clean_df, attrib_list=attributes_list, ent_list=entities_list):\n",
    "    # Identify duplicate predicted categories for each given review\n",
    "    # Use spacy cosine similarity measure to re-predict new categories\n",
    "    categories_new = []\n",
    "    for rid in process_df['rid'].unique():\n",
    "        if len(set(process_df.query(f'rid == \"{rid}\"')['predicted_category'])) < len(list(process_df.query(f'rid == \"{rid}\"')['predicted_category'])):\n",
    "            categories_new += set(process_df.query(f'rid == \"{rid}\"')['predicted_category'])\n",
    "\n",
    "            delta = len(list(process_df.query(f'rid == \"{rid}\"')['predicted_category'])) - len(set(process_df.query(f'rid == \"{rid}\"')['predicted_category']))\n",
    "            unique_cats = list(set(process_df.query(f'rid == \"{rid}\"')['predicted_category']))\n",
    "            duplicates = list(process_df.query(f'rid == \"{rid}\"')['predicted_category'])\n",
    "            [duplicates.remove(item) for item in unique_cats if item in duplicates]\n",
    "\n",
    "            # predict new category from ranked list of 20 most similar categories\n",
    "            review_text = clean_df.query(f'rid == \"{rid}\"')['text'].unique()[0]\n",
    "\n",
    "            entity_sim_dict = {}\n",
    "            for entity in ent_list[:5]:\n",
    "                entity_sim_dict[entity] = nlp(entity).similarity(nlp(review_text))\n",
    "            sorted_entities = sorted(((value,key) for key,value in entity_sim_dict.items()), reverse=True)\n",
    "\n",
    "            attrib_sim_dict = {}\n",
    "            for attrib in attrib_list[:4]:\n",
    "                attrib_sim_dict[attrib] = nlp(attrib).similarity(nlp(review_text))\n",
    "            sorted_attribs = sorted(((value,key) for key,value in attrib_sim_dict.items()), reverse=True)\n",
    "\n",
    "            unsorted_categories = {}\n",
    "            for i in sorted_entities:\n",
    "                for j in sorted_attribs:\n",
    "                    unsorted_categories[i[1]+'#'+j[1]] = i[0]*j[0]\n",
    "\n",
    "            sorted_categories = sorted(((value,key) for key,value in unsorted_categories.items()), reverse=True)\n",
    "            ranked_categories = [i[1] for i in sorted_categories]\n",
    "\n",
    "            # Only keep ranked_categories that are not already in unique_cats\n",
    "            [ranked_categories.remove(item) for item in ranked_categories if item in unique_cats]\n",
    "\n",
    "            for i in range(delta):\n",
    "                categories_new += [ranked_categories[i]]\n",
    "    \n",
    "        else:\n",
    "            categories_new += set(process_df.query(f'rid == \"{rid}\"')['predicted_category'])\n",
    "        \n",
    "    return categories_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identifying predicted categories for each review in Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating E#A category predictions given that the part 2 data contains the same sentences as part 1\n",
    "processed_df_p2['predicted_category'] = identify_p2_categories(train_reviews_df, train_reviews_df_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-predict duplicate predicted categories for given review\n",
    "# Note: cell takes ~45secs to run\n",
    "processed_df_p2['predicted_category'] = repredict_duplicate_cats(processed_df_p2, train_reviews_df_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort category predictions for more representative accuracy score (see example in function comments)\n",
    "processed_df_p2['predicted_category'] = sorted_predictions_p2(processed_df_p2)\n",
    "train_reviews_df_p2['predicted_category'] = sorted_predictions_p2(processed_df_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_a_labels_p2, e_a_predictions_p2, e_a_list_p2, e_a_label_dict_p2  = numerical_entity_attributes(processed_df_p2)\n",
    "category_train_accuracy_p2 = accuracy_score(e_a_labels_p2, e_a_predictions_p2)\n",
    "\n",
    "print(f\"Training set category predictions accuracy score: {category_train_accuracy_p2*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "reorder_columns = ['rid','processed_text','entity','entity_label',\n",
    "                  'attribute','attribute_label','category','predicted_category',\n",
    "                  'polarity','predicted_polarity']\n",
    "\n",
    "processed_df_p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_text_p2(text):\n",
    "    # Split input review into list of sentences.\n",
    "    split_input = []\n",
    "    for review in text:\n",
    "        split_input.append(sent_tokenize(review))\n",
    "        \n",
    "    return split_input\n",
    "\n",
    "def filter_sentiment_input_p2(text, cat, threshold=1):\n",
    "    # If predicted LAPTOP#GENERAL, keep entire review text.\n",
    "    # If one sentence in review is much more similar to predicted category\n",
    "    # than the next most similar sentence, keep only that sentence for sentiment prediction.\n",
    "    # Else keep the two most similar sentences for given category from review.\n",
    "\n",
    "    filtered_input_1 = []\n",
    "    filtered_input_2 = []\n",
    "    for idx, review in enumerate(text):\n",
    "        if cat[idx] == 'LAPTOP#GENERAL':\n",
    "            filtered_input_1.append(' '.join(review))\n",
    "            filtered_input_2.append(' '.join(review))\n",
    "        else:\n",
    "            cat_list = re.split('_|#', cat[idx])\n",
    "            cat_list = [item.title() if item != 'OS' else item for item in cat_list]\n",
    "            category = nlp(' '.join(cat_list))\n",
    "\n",
    "            sent_sim_dict = {}\n",
    "            for sentence in review:\n",
    "                sent_sim_dict[sentence] = category.similarity(nlp(sentence))\n",
    "            \n",
    "            sorted_sents = sorted(((value,key) for key,value in sent_sim_dict.items()), reverse=True)\n",
    "            \n",
    "            if len(sorted_sents) > 1:\n",
    "                # threshold hyperparameter determined on training set\n",
    "                if (sorted_sents[0][0] - sorted_sents[1][0]) < threshold:\n",
    "                    filtered_input_1.append(sorted_sents[0][1])\n",
    "                    filtered_input_2.append(sorted_sents[1][1])\n",
    "                else:\n",
    "                    filtered_input_1.append(sorted_sents[0][1])\n",
    "                    filtered_input_2.append(sorted_sents[0][1])\n",
    "            else:\n",
    "                filtered_input_1.append(sorted_sents[0][1])\n",
    "                filtered_input_2.append(sorted_sents[0][1])\n",
    "                \n",
    "    return filtered_input_1, filtered_input_2\n",
    "\n",
    "def p2_sentiment_predictions(input_1, input_2):\n",
    "    # make sentiment predictions\n",
    "    # polarity_labels_dict = {0: 'positive', 1: 'negative', 2: 'neutral', 3: 'conflict'} \n",
    "    N,D  = input_1.shape\n",
    "    Y_sentiment_pred = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        prediction_1 = sentiment_count_lr.predict(input_1[i,:])\n",
    "        prediction_2 = sentiment_count_lr.predict(input_2[i,:])\n",
    "        \n",
    "        if prediction_1 == prediction_2:\n",
    "            Y_sentiment_pred[i] = prediction_1\n",
    "\n",
    "        # if prediction is positive and negative\n",
    "        elif prediction_1 == 0 and prediction_2 == 1:\n",
    "            Y_sentiment_pred[i] = 3 # numerical value for conflict\n",
    "\n",
    "        # if prediction is positive and neutral\n",
    "        elif prediction_1 == 0 or prediction_2 == 0: \n",
    "            Y_sentiment_pred[i] = 0 # numerical value for positive\n",
    "\n",
    "        # if prediction is negative and neutral\n",
    "        elif prediction_1 == 1 or prediction_2 == 1: \n",
    "            Y_sentiment_pred[i] = 1 # numerical prediction for negative\n",
    "\n",
    "        # If prediction is neutral for both sentences\n",
    "        else:\n",
    "            Y_sentiment_pred[i] = 2 # numerical prediction for neutral\n",
    "        \n",
    "    return Y_sentiment_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making sentiment predictions for Part 2 training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label part 2 training sentiments\n",
    "sentiments_list_p2, polarity_label_dict_p2 = numerical_labels(processed_df_p2, 'polarity', 'polarity_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training labels for part 2 sentiment classification\n",
    "Y_sentiment_train_p2 = processed_df_p2['polarity_label']\n",
    "Y_sentiment_train_p2 = np.array(Y_sentiment_train_p2, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copy of data frame to populate with processed input text\n",
    "# specifically processed for sentiment prediction\n",
    "sentiment_processed_df_p2 = train_reviews_df_p2.copy()\n",
    "sentiment_processed_df_p2['text'] = split_input_text_p2(train_reviews_df_p2['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B. cell takes ~60secs to run\n",
    "sentiment_processed_df_p2['text_1'],sentiment_processed_df_p2['text_2'] = filter_sentiment_input_p2(sentiment_processed_df_p2['text'],\n",
    "                                        sentiment_processed_df_p2['predicted_category'],\n",
    "                                        threshold=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre process input features for sentiment classification\n",
    "# two sets of input features\n",
    "# If predicted sentiments for each set is different, predict conflict\n",
    "sentiment_processed_df_p2['processed_text_1'] = preprocess(tokenise(sentiment_processed_df_p2['text_1']))\n",
    "X_train_sentiment_p2_1 = count_vectorizer.transform(sentiment_processed_df_p2['processed_text_1'])\n",
    "\n",
    "sentiment_processed_df_p2['processed_text_2'] = preprocess(tokenise(sentiment_processed_df_p2['text_2']))\n",
    "X_train_sentiment_p2_2 = count_vectorizer.transform(sentiment_processed_df_p2['processed_text_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sentiment predictions on part 2 Training data\n",
    "Y_sentiment_train_pred_p2 = p2_sentiment_predictions(X_train_sentiment_p2_1, X_train_sentiment_p2_2)\n",
    "\n",
    "sentiment_train_accuracy_p2 = accuracy_score(Y_sentiment_train_p2, Y_sentiment_train_pred_p2)\n",
    "print(f\"Training set sentiment prediction accuracy score: {sentiment_train_accuracy_p2*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(Y_sentiment_train_p2, Y_sentiment_train_pred_p2, target_names=sentiments_list_p2)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews_df_p2 = part2_xml_to_df(filename='Laptops_Test_p2_gold.xml')\n",
    "train_reviews_df_p2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed_df_p2 = test_reviews_df_p2.copy()\n",
    "test_processed_df_p2['text'] = preprocess(tokenise(test_reviews_df_p2['text']))\n",
    "test_processed_df_p2 = test_processed_df_p2.rename(columns={\"text\": \"processed_text\"})\n",
    "test_processed_df_p2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating E#A category predictions given that the part 2 data contains the same sentences as part 1\n",
    "test_processed_df_p2['predicted_category'] = identify_p2_categories(test_reviews_df, test_reviews_df_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-predict duplicate predicted categories for given review\n",
    "# Note: cell takes ~20secs to run\n",
    "test_processed_df_p2['predicted_category'] = repredict_duplicate_cats(test_processed_df_p2, test_reviews_df_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort category predictions for more representative accuracy score (see example in function comments)\n",
    "test_processed_df_p2['predicted_category'] = sorted_predictions_p2(test_processed_df_p2)\n",
    "test_reviews_df_p2['predicted_category'] = sorted_predictions_p2(test_processed_df_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_a_labels_p2, e_a_predictions_p2, e_a_list_p2, e_a_label_dict_p2  = numerical_entity_attributes(test_processed_df_p2)\n",
    "\n",
    "cr = classification_report(e_a_labels_p2, e_a_predictions_p2, target_names=e_a_list_p2)\n",
    "print(\"(Part 2) Predicted E#A Category Classification Report (Test Set):\\n\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label part 2 test sentiments\n",
    "test_processed_df_p2['polarity_label'] = ''\n",
    "i = 0\n",
    "for label in sentiments_list_p2:\n",
    "    test_processed_df_p2.loc[test_processed_df_p2['polarity'] == label, 'polarity_label'] = i\n",
    "    polarity_label_dict_p2[i] = label\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test labels for part 2 sentiment classification\n",
    "Y_sentiment_test_p2 = test_processed_df_p2['polarity_label']\n",
    "Y_sentiment_test_p2 = np.array(Y_sentiment_test_p2, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copy of data frame to populate with processed input text\n",
    "# specifically processed for sentiment prediction\n",
    "test_sentiment_prep_df_p2 = test_reviews_df_p2.copy()\n",
    "test_sentiment_prep_df_p2['text'] = split_input_text_p2(test_reviews_df_p2['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B. cell takes ~30secs to run\n",
    "test_sentiment_prep_df_p2['text_1'],test_sentiment_prep_df_p2['text_2'] = filter_sentiment_input_p2(test_sentiment_prep_df_p2['text'],\n",
    "                                        test_sentiment_prep_df_p2['predicted_category'],\n",
    "                                        threshold=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre process input features for sentiment classification\n",
    "# two sets of input features\n",
    "# If predicted sentiments for each set is positive & negative, predict conflict\n",
    "test_sentiment_prep_df_p2['processed_text_1'] = preprocess(tokenise(test_sentiment_prep_df_p2['text_1']))\n",
    "X_test_sentiment_p2_1 = count_vectorizer.transform(test_sentiment_prep_df_p2['processed_text_1'])\n",
    "\n",
    "test_sentiment_prep_df_p2['processed_text_2'] = preprocess(tokenise(test_sentiment_prep_df_p2['text_2']))\n",
    "X_test_sentiment_p2_2 = count_vectorizer.transform(test_sentiment_prep_df_p2['processed_text_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sentiment predictions on part 2 test data\n",
    "Y_sentiment_test_pred_p2 = p2_sentiment_predictions(X_test_sentiment_p2_1, X_test_sentiment_p2_2)\n",
    "\n",
    "# Produce classification report\n",
    "cr = classification_report(Y_sentiment_test_p2, Y_sentiment_test_pred_p2, target_names=sentiments_list_p2)\n",
    "print(\"(Part 2) Predicted Sentiment Classification Report (Test Set):\\n\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed_df_p2['predicted_polarity'] = convert_numerical_predictions(Y_sentiment_test_pred_p2, polarity_label_dict_p2)\n",
    "test_reviews_df_p2['predicted_polarity'] = convert_numerical_predictions(Y_sentiment_test_pred_p2, polarity_label_dict_p2)\n",
    "\n",
    "test_reviews_df_p2[['text','category','predicted_category','polarity','predicted_polarity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the % of predictions where both the category AND polarity were predicted correct\n",
    "combined_accuracy_p2 = overall_accuracy_absa(test_reviews_df_p2)\n",
    "print(\"For Part 2 test data:\")\n",
    "print(combined_accuracy_p2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5417c8c9055c1fa8caa20bdcdcb9a94277d5725cf87b106d0a354dab992ca3fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
